{
    "componentChunkName": "component---src-pages-openshift-tune-xmlserver-tune-xmlserver-mdx",
    "path": "/openshift/tune_xmlserver/tune_xmlserver/",
    "result": {"pageContext":{"frontmatter":{"title":"Tuning the XML server pods","description":"Tuning the XML server pods"},"relativePagePath":"/openshift/tune_xmlserver/tune_xmlserver.mdx","titleType":"page","MdxNode":{"id":"aa2e9959-d283-50c3-8ceb-ab9974ca822d","children":[],"parent":"98d1d225-409d-5c1a-9560-866d06904eb0","internal":{"content":"---\ntitle: Tuning the XML server pods\ndescription: Tuning the XML server pods\n---\n\n<AnchorLinks small>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Replicas</AnchorLink>\n  <AnchorLink>Pod requests and limits</AnchorLink>\n  <AnchorLink>Tuning the XML server running in a pod</AnchorLink>\n  <AnchorLink>Assessing XML server performance</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThe following information is specific for the tuning of the SPM XML server on OpenShift and IBM Kubernetes Services.\n\nAdditionally, the general XML server tuning information is available in the [Tuning the XML server](/common_task/xmlservertuning) section of this guide.\n\n## Pod replica count\n\nThe number of XML Server replicas in a deployment can be specified globally via `xmlserver.replicaCount`, which can be used to amend the default single instance.\n\n## Pod requests and limits\n\nWhen assessing XML server performance and the setting of XML server pod requests and limits, note that additional replicas are automatically load balanced via the `release-xmlserver` service.\n\nThe XML server pod `cpu` and `memory` resource requests and limits can be specified by setting `cpu`: and `memory`: values for the `xmlserver.resources` keys.\n\nThe following example illustrates how to set the number of replicas to `6`, with resources requests for the `cpu` of 1 and a `memory` of 1024Mi, and with resources limits for the `cpu` of 1 and a `memory` of 1024Mi:\n\n```yaml\nxmlserver:\n  replicaCount: 6\n  resources:\n    requests:\n      cpu: 1\n      memory: 1024Mi\n    limits:\n      cpu: 1\n      memory: 1024Mi\n```\n\n<InlineNotification kind=\"warning\">\n\nFor the Guaranteed QoS, SPM highly recommends that a pod must have **both** CPU and Memory requests and limits set to equal values. If requests & limits are different for a given pod, it would be eligible for rescheduling in case of resource pressure on its node.\n\nFor more information please review [Configure Quality of Service for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed).\n\n</InlineNotification>\n\n## Tuning the XML server running in a pod\n\nThe XML server provides two areas for tuning:\n\n1. The XML server configuration\n1. The JVM in which the XML server runs\n\nSpecific settings for each of these areas are available via Helm chart overrides.  For more information about Helm charts see the IBM Cúram Social Program Management on Kubernetes Runbook:\n\n* The [Configuration Reference](https://ibm.github.io/spm-kubernetes/deployment/config-reference/#xml-server) section lists the Helm chart settings and their default values for the configuration that follows;\n* The [Deploying Helm charts](https://ibm.github.io/spm-kubernetes/deployment/hc_deployment/) section describes how to specify Helm chart overrides in an override file.\n\n### XML server configuration\n\nThese are configuration settings of the XML server, accessible via Helm chart overrides, that can be used to tune its performance:\n\n* XML server thread pool size - This setting essentially controls the amount of work the XML server can do.\n* XML server thread pool queue size - This setting controls the number of requests that are queued inside the XML server as opposed to the TCP backlog queue.\n* XML server socket timeout - This setting (expressed as milliseconds) determines the length of time the XML server will wait for a socket request to complete.\n\nThe SPM Helm charts are coded to be able to specify `threadPoolSize`, `threadPoolQueueSize` and `socketTimeout`. For instance, this shows the default settings for `xmlserver` application pods:\n\n```yaml\nxmlserver:\n  config:\n    threadPoolSize: 5\n    threadPoolQueueSize: 200\n    socketTimeout: 60000\n```\n\n### XML server JVM\n\nThese are Java JVM settings, accessbile via Helm chart overrides, that can be used to tune its performance:\n\n* Java maximum heap - This setting maps to the the `maxmemory` attribute of the Ant `<java>` task in the XML server `xmlserver.xml` Ant script, which is invoked when the pod is started. (Note: Ant's `maxmemory` attribute overrides the JVM `-Xmx` setting.)\n* Java thread stack size - This maps to  the specification of the JVM stack size argument, `-Xss`, specified as an Ant property override; for example: `-Djava.thread.stack.size=\"-Xss4m\"`\n\nThe SPM Helm charts are coded to be able to specify the JVM `maxMemory` and `threadStackSize`. For instance, this shows the default settings for `xmlserver` application pods:\n\n```yaml\nxmlserver:\n  jvm:\n    maxMemory: '-Djava.maxmemory=768m'\n    threadStackSize: '-Djava.thread.stack.size=-Xss4m'\n```\n\n## Assessing XML server performance\n\nThe following data can help you assess when tuning may be called for (note limited data availability when not using persistence):\n\n* JVM verbose garbage collection data is your primary source of information on whether the Java heap is sized appropriately.\n  * Availability of data:\n    * When persistence is configured `verbosegc.log` can be found in the persistent `gc` folder of the XML server pod.\n    * When persistence is not configured `verbosegc.log` can be found in the `/opt/ibm/Curam/xmlserver/tmp` folder of the XML server pod during its execution.\n  * To process the data you can use the\n    [IBM Garbage Collection and Memory Visualizer (GCMV)](https://www.ibm.com/support/pages/garbage-collection-and-memory-visualizer-gcmv-2701-ibm-support-assistant-team-server) tool.\n* XML server thread pool worker data is your primary source of information on how the XML server is performing, which can be impacted by, among other factors, how the JVM is performing.\n  * Availability of data:\n    * When persistence is configured the `ThreadPoolWorker-*` files can be found in the persistent `stats` folder of the XML server pod after it has been stopped.\n    * When persistence is not configured there is no data available.\n  * To process the data you can use text-based tools such as [awk](https://en.wikipedia.org/wiki/AWK) as the data in the `ThreadPoolWorker-*` files\n    is tab-delimited, one file per thread (as specified by [<THREAD_POOL_SIZE>](/common_task/xmlservertuning#tuning-and-configuration-changes)).\n","type":"Mdx","contentDigest":"4c81f00540889718c3321e083ae58d7c","owner":"gatsby-plugin-mdx","counter":132},"frontmatter":{"title":"Tuning the XML server pods","description":"Tuning the XML server pods"},"exports":{},"rawBody":"---\ntitle: Tuning the XML server pods\ndescription: Tuning the XML server pods\n---\n\n<AnchorLinks small>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>Replicas</AnchorLink>\n  <AnchorLink>Pod requests and limits</AnchorLink>\n  <AnchorLink>Tuning the XML server running in a pod</AnchorLink>\n  <AnchorLink>Assessing XML server performance</AnchorLink>\n</AnchorLinks>\n\n## Overview\n\nThe following information is specific for the tuning of the SPM XML server on OpenShift and IBM Kubernetes Services.\n\nAdditionally, the general XML server tuning information is available in the [Tuning the XML server](/common_task/xmlservertuning) section of this guide.\n\n## Pod replica count\n\nThe number of XML Server replicas in a deployment can be specified globally via `xmlserver.replicaCount`, which can be used to amend the default single instance.\n\n## Pod requests and limits\n\nWhen assessing XML server performance and the setting of XML server pod requests and limits, note that additional replicas are automatically load balanced via the `release-xmlserver` service.\n\nThe XML server pod `cpu` and `memory` resource requests and limits can be specified by setting `cpu`: and `memory`: values for the `xmlserver.resources` keys.\n\nThe following example illustrates how to set the number of replicas to `6`, with resources requests for the `cpu` of 1 and a `memory` of 1024Mi, and with resources limits for the `cpu` of 1 and a `memory` of 1024Mi:\n\n```yaml\nxmlserver:\n  replicaCount: 6\n  resources:\n    requests:\n      cpu: 1\n      memory: 1024Mi\n    limits:\n      cpu: 1\n      memory: 1024Mi\n```\n\n<InlineNotification kind=\"warning\">\n\nFor the Guaranteed QoS, SPM highly recommends that a pod must have **both** CPU and Memory requests and limits set to equal values. If requests & limits are different for a given pod, it would be eligible for rescheduling in case of resource pressure on its node.\n\nFor more information please review [Configure Quality of Service for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed).\n\n</InlineNotification>\n\n## Tuning the XML server running in a pod\n\nThe XML server provides two areas for tuning:\n\n1. The XML server configuration\n1. The JVM in which the XML server runs\n\nSpecific settings for each of these areas are available via Helm chart overrides.  For more information about Helm charts see the IBM Cúram Social Program Management on Kubernetes Runbook:\n\n* The [Configuration Reference](https://ibm.github.io/spm-kubernetes/deployment/config-reference/#xml-server) section lists the Helm chart settings and their default values for the configuration that follows;\n* The [Deploying Helm charts](https://ibm.github.io/spm-kubernetes/deployment/hc_deployment/) section describes how to specify Helm chart overrides in an override file.\n\n### XML server configuration\n\nThese are configuration settings of the XML server, accessible via Helm chart overrides, that can be used to tune its performance:\n\n* XML server thread pool size - This setting essentially controls the amount of work the XML server can do.\n* XML server thread pool queue size - This setting controls the number of requests that are queued inside the XML server as opposed to the TCP backlog queue.\n* XML server socket timeout - This setting (expressed as milliseconds) determines the length of time the XML server will wait for a socket request to complete.\n\nThe SPM Helm charts are coded to be able to specify `threadPoolSize`, `threadPoolQueueSize` and `socketTimeout`. For instance, this shows the default settings for `xmlserver` application pods:\n\n```yaml\nxmlserver:\n  config:\n    threadPoolSize: 5\n    threadPoolQueueSize: 200\n    socketTimeout: 60000\n```\n\n### XML server JVM\n\nThese are Java JVM settings, accessbile via Helm chart overrides, that can be used to tune its performance:\n\n* Java maximum heap - This setting maps to the the `maxmemory` attribute of the Ant `<java>` task in the XML server `xmlserver.xml` Ant script, which is invoked when the pod is started. (Note: Ant's `maxmemory` attribute overrides the JVM `-Xmx` setting.)\n* Java thread stack size - This maps to  the specification of the JVM stack size argument, `-Xss`, specified as an Ant property override; for example: `-Djava.thread.stack.size=\"-Xss4m\"`\n\nThe SPM Helm charts are coded to be able to specify the JVM `maxMemory` and `threadStackSize`. For instance, this shows the default settings for `xmlserver` application pods:\n\n```yaml\nxmlserver:\n  jvm:\n    maxMemory: '-Djava.maxmemory=768m'\n    threadStackSize: '-Djava.thread.stack.size=-Xss4m'\n```\n\n## Assessing XML server performance\n\nThe following data can help you assess when tuning may be called for (note limited data availability when not using persistence):\n\n* JVM verbose garbage collection data is your primary source of information on whether the Java heap is sized appropriately.\n  * Availability of data:\n    * When persistence is configured `verbosegc.log` can be found in the persistent `gc` folder of the XML server pod.\n    * When persistence is not configured `verbosegc.log` can be found in the `/opt/ibm/Curam/xmlserver/tmp` folder of the XML server pod during its execution.\n  * To process the data you can use the\n    [IBM Garbage Collection and Memory Visualizer (GCMV)](https://www.ibm.com/support/pages/garbage-collection-and-memory-visualizer-gcmv-2701-ibm-support-assistant-team-server) tool.\n* XML server thread pool worker data is your primary source of information on how the XML server is performing, which can be impacted by, among other factors, how the JVM is performing.\n  * Availability of data:\n    * When persistence is configured the `ThreadPoolWorker-*` files can be found in the persistent `stats` folder of the XML server pod after it has been stopped.\n    * When persistence is not configured there is no data available.\n  * To process the data you can use text-based tools such as [awk](https://en.wikipedia.org/wiki/AWK) as the data in the `ThreadPoolWorker-*` files\n    is tab-delimited, one file per thread (as specified by [<THREAD_POOL_SIZE>](/common_task/xmlservertuning#tuning-and-configuration-changes)).\n","fileAbsolutePath":"/home/travis/build/IBM/spm-performance-tuning/src/pages/openshift/tune_xmlserver/tune_xmlserver.mdx"}}},
    "staticQueryHashes": ["1364590287","137577622","2102389209","2456312558","2610115429","2746626797","3037994772","768070550"]}