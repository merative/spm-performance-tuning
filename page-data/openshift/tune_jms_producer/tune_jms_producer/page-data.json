{
    "componentChunkName": "component---src-pages-openshift-tune-jms-producer-tune-jms-producer-mdx",
    "path": "/openshift/tune_jms_producer/tune_jms_producer/",
    "result": {"pageContext":{"frontmatter":{"title":"Tuning the JMS producer pods","description":"Tuning the JMS producer pods"},"relativePagePath":"/openshift/tune_jms_producer/tune_jms_producer.mdx","titleType":"page","MdxNode":{"id":"bc9c32ee-9b01-545d-926b-a25a96e0b966","children":[],"parent":"0cd9adaa-2b92-5993-b943-f70ca8b3d95a","internal":{"content":"---\ntitle: Tuning the JMS producer pods\ndescription: Tuning the JMS producer pods\n---\n\n<AnchorLinks small>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>SPM configuration reference overrides</AnchorLink>\n  <AnchorLink>Pod replica count</AnchorLink>\n  <AnchorLink>Pod requests and limits</AnchorLink>\n  <AnchorLink>Liberty thread pool</AnchorLink>\n  <AnchorLink>Liberty JDBC configuration tuning</AnchorLink>\n  <AnchorLink>Liberty JMS configuration tuning</AnchorLink>\n  <AnchorLink>Liberty JVM heap</AnchorLink>\n  <AnchorLink>Liberty HTTP session replication</AnchorLink>\n</AnchorLinks>\n\n<InlineNotification>\n\n**Note:**  There are failover and performance considerations to be weighed which customers should evaluate and test, specific to their requirements and environment.\n\nFor more information see:\n\n* IBM Documentation: [Configuring Liberty session persistence to a database](https://www.ibm.com/docs/en/was-liberty/base?topic=manually-configuring-liberty-session-persistence-database).\n* IBM Documentation: [HTTP Session Database (httpSessionDatabase)](https://www.ibm.com/docs/en/was-liberty/base?topic=configuration-httpsessiondatabase).\n* Additional Liberty tuning advice in the HTTP Sessions section of the [IBM WebSphere Application Server Performance Cookbook](https://publib.boulder.ibm.com/httpserv/cookbook/WebSphere_Application_Server-WAS_Liberty.html#http-session-database-persistence).\n\n</InlineNotification>\n\n## Overview\n\nThe term **JMS producer** refers to the functional structure of SPM applications (for example, `curam`, `rest`, etc.)\nas deployed in OpenShift such that the client portion of the application is isolated from its server side functionality.\nThat is, SPM clients produce JMS messages that perform asynchronous operations, which are consumed by corresponding **JMS consumer** pods.\nFor more information please review [Transaction isolation](https://www.ibm.com/docs/en/spm/8.0.0?topic=architecture-transaction-isolation).\n\nBenefits of this separation include:\n\n* Allowing unique tuning that caters to the type of work done in each pod type\n* Separation of JMS/MQ put and get functionality\n* Scale JMS producer independently of the JMS consumer\n* Isolation of the Kubernetes services\n\n## SPM configuration reference overrides\n\n<InlineNotification>\n\nSample override files are available in the [spm-kubernetes/static/resources](https://github.com/IBM/spm-kubernetes/tree/master/static/resources) folder.\n\nThese samples illustrate basic deployment settings for several environments.   All the settings are listed in the [Configuration Reference](https://ibm.github.io/spm-kubernetes/deployment/config-reference/) topic.\n\n</InlineNotification>\n\nHelm charts allow for flexibility in specifying tuning settings.\nIn SPM deployments, tuning configuration settings can be made globally, by deployment type (e.g. producer), or by application (e.g. `curam`).\n\nThe list below that illustrates this tuning flexibility where `<applicationID>` is replaced by the lower-case EAR file basename; that is, in the case of `Curam.ear` use the value `curam`:\n\n* `apps.global.tuningDefaults` - global tuning\n* `global.apps.config.<applicationID>.producerTuning` - dictionary containing tuning values specific to the producer pods for that application\n* `global.apps.config.<applicationID>` - for the following keys:\n  * `jvm` - Liberty JVM heap and other settings\n  * `replicaCount` - the number of replicas\n  * `resources` - varies by application\n  * The various keys from the `apps.tuningDefaults` dictionary.\n\nFurther, `apps.tuningDefaults.resources` allows for fine tuning of a pod's resources, overriding `global.apps.config.<applicationID>.resources`.\n\n## Pod replica count\n\nThe purpose of specifying a replica count is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.\n\nThe OpenShift and Kubernetes documentation provide more information about replicas:\n\n* OpenShift: [Replication controllers](https://docs.openshift.com/container-platform/4.7/applications/deployments/what-deployments-are.html#deployments-replicationcontrollers_what-deployments-are)\n* Kubernetes: [ReplicaSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)\n\nThe number of replicas in a deployment can be specified globally via `apps.replicaCount` or via `global.apps.config.<applicationID>.replicaCount`: where `<applicationID>` is replaced by the lower-case EAR file basename (e.g., `curam`, `citizenportal`, `rest`, etc).\n\nThe default is a single replica.\n\nFor example, an override file to specify 2 `curam` replicas and 4 `rest` replicas:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        replicaCount: 2\n      rest:\n        replicaCount: 4\n...\n```\n\nThe number of replicas can also be specified at a more granular level for producer and consumer deployments.\nFor example, an override file to specify three `curam producer` replicas and six `curam consumer` replicas:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          replicaCount: 3\n        consumerTuning:\n          replicaCount: 6\n...\n```\n\n## Pod requests and limits\n\nThe OpenShift and Kubernetes documentation provide more information about requests and limits:\n\n* OpenShift: [Resource quotas per project](https://docs.openshift.com/container-platform/4.7/applications/quotas/quotas-setting-per-project.html)\n* Kubernetes: [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)\n\nThe SPM Helm charts are coded to be able to specify `cpu` and `memory` requests and limits. For instance, this shows the default settings for `curam` application pods:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        resources:\n          limits:\n            cpu: 2\n            memory: 4Gi\n          requests:\n            cpu: 2\n            memory: 4Gi\n...\n```\n\n<InlineNotification kind=\"warning\">\n\nFor the Guaranteed QoS, SPM highly recommends that a pod must have **both** CPU and Memory requests and limits set to equal values. If requests & limits are different for a given pod, it would be eligible for rescheduling in case of resource pressure on its node.\n\nFor more information please review [Configure Quality of Service for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed).\n\n</InlineNotification>\n\nThese requests and limits can also be specified at a more granular level for producer and consumer deployments.\n\nFor example:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          resources:\n            limits:\n              cpu: 2\n              memory: 3584Mi\n            requests:\n              cpu: 2\n              memory: 3584Mi\n        consumerTuning:\n          resources:\n            limits:\n              cpu: 2\n              memory: 2048Mi\n            requests:\n              cpu: 2\n              memory: 2048Mi\n...\n```\n\n## Liberty thread pool\n\nThe total number of threads that SPM uses in a producer pod can be set initially to `(requested_cpu * 2)`. Setting the number of threads to twice the number of cores is based on experience that processing in SPM is usually split relatively equally between I/O and CPU.\n\nThe SPM Helm charts allow for overriding the WebSphere Liberty executor thread pool minimum (`coreThreads`) and maximum (`maxThreads`) settings either globally\n(e.g. `apps.tuningDefaults.coreThreads`) or per application as per the [Initial Tuning Settings](https://ibm.github.io/spm-kubernetes/resources/tuning-values.yaml).\n\nFor example, a tuning specification of 8 threads for `curam` producer pods:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          coreThreads: 8\n          maxThreads: 8\n...\n```\n\nThe `coreThreads` and `maxThreads` values map to the pod's WebSphere Liberty configuration in `/config/server.xml` and the values are populated via `/config/server.env`.\n\nFor example:\n\n```xml\n<server>\n  ...\n  <executor coreThreads=\"${env.EX_CORE_THREADS}\" maxThreads=\"${env.EX_MAX_THREADS}\" />\n</server>\n```\n\n## Liberty JDBC configuration tuning\n\n### Data source: `jdbc/curamdb`\n\nA Social Program Management (SPM) transaction can require two JDBC connections, one for the transaction itself and another one for the key server.\n\nTherefore, size the `jdbc/curamdb` data source connection pool to prevent deadlocks, with more connections available than threads that SPM uses.\n\n### Data source: `jdbc/curamtimerdb`\n\nThe EJB timer service is used by all SPM transactions, but only once per transaction, in our application infrastructure and at the very start of an SPM transaction. Currently, no reference to or usage of this service exists after the very start of an SPM transaction.\n\nYou can tune the size of the jdbc/curamtimerdb data source connection pool to be the same size as the number of threads, which would ensure that no contention can occur on the pool.\nHowever, given that the time that is spent using the EJB timer service is typically short compared to the duration of the transactions, a smaller size should work well without significant contention.\nSo our advice is to start with the default size, monitor the system, and then increase the size if evidence exists of a significant contention under normal conditions.\n\n### Statement Cache Size\n\nAs a starting value for SPM, set the data source prepared statement cache size for `jdbc/curamdb` to `1000`.\nThen, monitor the cache use and increase it if discards occur. In our experience, preventing discards can increase throughput by up to 20%.\n\n<InlineNotification>\n\n**Note:** While we recommend an initial value of 1000 for the prepared statement cache to prevent discards,\nthis value can be too high for SPM-based systems that have many threads and that are memory constrained.\nIn that case, it is recommended to review the SQLStats from the JMX Stats, from either load tests or production.\nThen, use a simple heuristic based on the distribution of SQL executions to find a smaller cache size that covers around\n90% of SQL executions from the application and gives a better balance between system performance and Java™ heap utilization.\nHowever, monitor prepared statement cache discards, system performance, and heap utilization, and adjust the cache size further as needed.\n</InlineNotification>\n\n### JDBC configuration tuning\n\nThe WebSphere Liberty [JDBC configuration](https://www.ibm.com/docs/en/was-liberty/base?topic=configuration-datasource) is tunable for each of these database definitions:\n\n|Database definitions | Description |\n|:- | :- |\n|`curamdb`|Used by SPM applications|\n|`curamtimerdb` | Used by the [SPM timer infrastructure](https://www.ibm.com/docs/en/spm/8.0.0?topic=behavior-cram-timer) |\n|`curamsessdb`| Used for WebSphere Liberty's [HTTP session replication](/openshift/tune_jms_producer/tune_jms_producer#liberty-http-session-replication)|\n\nThe following yaml keys are provided for tuning the WebSphere Liberty JDBC configuration:\n\n* _maxPoolSize_ -  Maximum number of database connections; Helm chart default: `8`\n  * `curamdb_maxPoolSize` = `(( max_threads * 2 ) + 1))`\n  * `curamtimerdb_maxPoolSize` = `( max_threads + 1)`\n  * `curamsessdb_maxPoolSize`= `( max_threads + 1 )`\n* _numConnectionsPerThreadLocal_ - Number of connections to the database to be cached for each thread; Helm chart default: `2`\n\n  <InlineNotification>\n\n  **Note:** For SPM _numConnectionsPerThreadLocal_ should always be set to `2`. `1` connection for the `keyserver` and `1` connection for the SPM database\n\n  </InlineNotification>\n\n  * `curamdb_numConnectionsPerThreadLocal` = `2`\n  * `curamtimerdb_numConnectionsPerThreadLocal` = `2`\n  * `curamsessdb_numConnectionsPerThreadLocal` = `2`\n* _purgePolicy_ - Connections to be destroyed in the pool when a stale connection is detected; Helm chart default: `EntirePool`\n  * `curamdb_purgePolicy` = `EntirePool`\n  * `curamtimerdb_purgePolicy` = `EntirePool`\n  * `curamsessdb_purgePolicy` = `EntirePool`\n* _statementCacheSize_ - Maximum number of cached statements per connection; Helm chart default: `1000`\n  * `curamdb_statementCacheSize` = `1000`\n  * `curamtimerdb_statementCacheSize` = `1000`\n  * `curamsessdb_statementCacheSize` = `1000`\n\nThe SPM Helm charts allow for overriding the JDBC configuration either globally\n(e.g., `apps.tuningDefaults.curamdb_maxPoolSize`) or per application as per the [Initial Tuning Settings](https://ibm.github.io/spm-kubernetes/resources/tuning-values.yaml) examples provided.\nHere we illustrate tuning settings for curamdb in curam producer pods:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          # Curam Producer Database Settings\n          curamdb_maxPoolSize: 17\n          curamdb_numConnectionsPerThreadLocal: 2\n          curamdb_purgePolicy: EntirePool\n          curamdb_statementCacheSize: 1000\n\n          curamtimerdb_maxPoolSize: 9\n          curamtimerdb_numConnectionsPerThreadLocal: 2\n          curamtimerdb_purgePolicy: EntirePool\n          curamtimerdb_statementCacheSize: 1000\n\n          curamsessdb_maxPoolSize: 9\n          curamsessdb_numConnectionsPerThreadLocal: 2\n          curamsessdb_purgePolicy: EntirePool\n          curamsessdb_statementCacheSize: 1000\n...\n```\n\nThe various keys in the preceding list map to the pod's WebSphere Liberty configuration in `/config/adc_conf/server_resources_jdbc.xml` and the setting values are populated via `/config/server.env`.\nFor example, showing the relevant parts of the `curamdb` configuration:\n\n```xml\n<server>\n  <dataSource id=\"curamdb\" jndiName=\"jdbc/curamdb\"\n    statementCacheSize=\"${env.DS_CURAMDB_CACHE_SIZE}\">\n    <connectionManager\n      maxPoolSize=\"${env.CM_CURAMDB_MAX_POOL_SIZE}\"\n      numConnectionsPerThreadLocal=\"${env.CM_CURAMDB_CONN_PER_THREAD}\"\n      purgePolicy=\"${env.CM_CURAMDB_PURGE_POLICY}\"\n    />\n  </dataSource>\n</server>\n```\n\n## Liberty JMS configuration tuning\n\nThe WebSphere Liberty JMS configuration is tunable for the JMS [connection manager settings](https://www.ibm.com/docs/en/was-liberty/base?topic=configuration-jmsconnectionfactory#connectionManager) associated with the `CuramQueueConnectionFactory`\n\n### JMS connection manager tuning\n\nThe following JMS connection manager settings, associated with the `CuramQueueConnectionFactory`, can be tuned:\n\n|Connection manager setting|Yaml default comes from|Yaml key used if specific|Description|\n|--------------------------|-----------------------|-------------------------|-----------|\n|`maxPoolSize`|`max_thread + 1`|`apps.<applicationID>.producerTuning.mqMaxPoolSize`|Specifies the maximum number of physical connections for the connection pool|\n|`minPoolSize`|`max_thread + 1`|`apps.<applicationID>.producerTuning.mqMinPoolSize`|Specifies the minimum number of physical connections for the connection pool|\n|`numConnectionsPerThreadLocal`| `6`|`apps.<applicationID>.producerTuning.mqNumConnectionsPerThreadLocal`|Specifies the number of connections to cache for each executor thread|\n|`maxConnectionsPerThread`| `6`|`apps.<applicationID>.producerTuning.maxJMSConnectionsPerThread`|Limits the number of open connections on each thread|\n\nThe keys in the preceding table map to the pod's WebSphere Liberty configuration in `/config/adc_conf/server_resources_messaging.xml`\nand the setting values are populated via `/config/server.env` as shown in this WebSphere Liberty configuration fragment:\n\n```xml\n<server>\n  ...\n  <connectionManager\n    id=\"ConMgr6\"\n    maxPoolSize=\"${env.CM_MQ_MAXPOOLSIZE}\"\n    minPoolSize=\"${env.CM_MQ_MINPOOLSIZE}\"\n    numConnectionsPerThreadLocal=\"${env.CM_JMS_NUM_CONNECTIONS_PER_THREAD_LOCAL}\"\n    maxConnectionsPerThread=\"${env.CM_JMS_MAX_CONNECTIONS_PER_THREAD}\"\n  />\n</server>\n\n```\n\nThe SPM Helm charts allow for overriding the JMS configuration either globally (e.g. `apps.tuningDefaults.maxThreads`) or per application as per the [Initial Tuning Settings](https://ibm.github.io/spm-kubernetes/resources/tuning-values.yaml) examples provided.\nHere we illustrate tuning settings for curamdb in curam producer pods:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          mqMaxPoolSize: 9\n          mqMinPoolSize: 9\n          mqNumConnectionsPerThreadLocal: 6\n          maxJMSConnectionsPerThread: 6\n...\n```\n\n## Liberty JVM heap\n\nWebSphere Liberty JVM options are specified via a yaml array in `global.apps.config.<applicationID>.jvm` for all pod types of an application or for specific pod types such as producer, via the `global.apps.config.<applicationID>.producerTuning`\tdictionary of tuning values.\n\n<InlineNotification>\n\n**Note:** SPM recommends minimum memory setting per producer pod of `3584Mi`.\n\n</InlineNotification>\n\nStart with the following settings:\n\n* For a given consumer pod where `memory requests` = `3584Mi`, tune the JVM heap size by using the following example:\n\n```\n  -Xmx = 2560M\n  -Xms = 2560M\n  -Xmn = 1536M\n```\n\nFragment showing JVM settings for the `curam` JMS producer pods (as distinct from the `curam` JMS producer pods):\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          jvm: ['-Xms2560M','-Xmx2560M','-Xmn1536M']\n```\n\nThese settings are placed in the pod's `/config/jvm.options` file at deployment, for instance:\n\n```\n-Xms2560M\n-Xmx2560M\n-Xmn1536M\n```\n\n## Liberty HTTP session replication\n\nSPM deployed in Kubernetes uses WebSphere Liberty HTTP session replication for failover.\nThis replication is done using the database as the persistence and sharing mechanism for HTTP sessions.\nIn our performance tests we have seen at least an order of magnitude improvement in service times from the REST producer pod by switching the write frequency of the HTTP session replication from \"End of Servlet service\" to \"Time based\".\n\nWhen using \"End of Servlet service\", for each HTTP request arriving to the pod, the HTTP session is first read from the database, then the SPM code is executed,\nand lastly the HTTP session is updated in the database before the HTTP response is sent.\nWhen using \"Time based\", HTTP requests incur a much smaller overhead as the HTTP sessions are updated in the database asynchronously to the HTTP requests.\n\nExample in context:\n\n```xml\n<server>\n  <httpSessionDatabase\n    ....\n    skipIndexCreation=\"false\"\n    writeFrequency=\"TIME_BASED_WRITE\"\n    writeInterval=\"2m\"\n  />\n</server>\n```\n","type":"Mdx","contentDigest":"9d538a4e195c51cef946e761c1dee26a","owner":"gatsby-plugin-mdx","counter":130},"frontmatter":{"title":"Tuning the JMS producer pods","description":"Tuning the JMS producer pods"},"exports":{},"rawBody":"---\ntitle: Tuning the JMS producer pods\ndescription: Tuning the JMS producer pods\n---\n\n<AnchorLinks small>\n  <AnchorLink>Overview</AnchorLink>\n  <AnchorLink>SPM configuration reference overrides</AnchorLink>\n  <AnchorLink>Pod replica count</AnchorLink>\n  <AnchorLink>Pod requests and limits</AnchorLink>\n  <AnchorLink>Liberty thread pool</AnchorLink>\n  <AnchorLink>Liberty JDBC configuration tuning</AnchorLink>\n  <AnchorLink>Liberty JMS configuration tuning</AnchorLink>\n  <AnchorLink>Liberty JVM heap</AnchorLink>\n  <AnchorLink>Liberty HTTP session replication</AnchorLink>\n</AnchorLinks>\n\n<InlineNotification>\n\n**Note:**  There are failover and performance considerations to be weighed which customers should evaluate and test, specific to their requirements and environment.\n\nFor more information see:\n\n* IBM Documentation: [Configuring Liberty session persistence to a database](https://www.ibm.com/docs/en/was-liberty/base?topic=manually-configuring-liberty-session-persistence-database).\n* IBM Documentation: [HTTP Session Database (httpSessionDatabase)](https://www.ibm.com/docs/en/was-liberty/base?topic=configuration-httpsessiondatabase).\n* Additional Liberty tuning advice in the HTTP Sessions section of the [IBM WebSphere Application Server Performance Cookbook](https://publib.boulder.ibm.com/httpserv/cookbook/WebSphere_Application_Server-WAS_Liberty.html#http-session-database-persistence).\n\n</InlineNotification>\n\n## Overview\n\nThe term **JMS producer** refers to the functional structure of SPM applications (for example, `curam`, `rest`, etc.)\nas deployed in OpenShift such that the client portion of the application is isolated from its server side functionality.\nThat is, SPM clients produce JMS messages that perform asynchronous operations, which are consumed by corresponding **JMS consumer** pods.\nFor more information please review [Transaction isolation](https://www.ibm.com/docs/en/spm/8.0.0?topic=architecture-transaction-isolation).\n\nBenefits of this separation include:\n\n* Allowing unique tuning that caters to the type of work done in each pod type\n* Separation of JMS/MQ put and get functionality\n* Scale JMS producer independently of the JMS consumer\n* Isolation of the Kubernetes services\n\n## SPM configuration reference overrides\n\n<InlineNotification>\n\nSample override files are available in the [spm-kubernetes/static/resources](https://github.com/IBM/spm-kubernetes/tree/master/static/resources) folder.\n\nThese samples illustrate basic deployment settings for several environments.   All the settings are listed in the [Configuration Reference](https://ibm.github.io/spm-kubernetes/deployment/config-reference/) topic.\n\n</InlineNotification>\n\nHelm charts allow for flexibility in specifying tuning settings.\nIn SPM deployments, tuning configuration settings can be made globally, by deployment type (e.g. producer), or by application (e.g. `curam`).\n\nThe list below that illustrates this tuning flexibility where `<applicationID>` is replaced by the lower-case EAR file basename; that is, in the case of `Curam.ear` use the value `curam`:\n\n* `apps.global.tuningDefaults` - global tuning\n* `global.apps.config.<applicationID>.producerTuning` - dictionary containing tuning values specific to the producer pods for that application\n* `global.apps.config.<applicationID>` - for the following keys:\n  * `jvm` - Liberty JVM heap and other settings\n  * `replicaCount` - the number of replicas\n  * `resources` - varies by application\n  * The various keys from the `apps.tuningDefaults` dictionary.\n\nFurther, `apps.tuningDefaults.resources` allows for fine tuning of a pod's resources, overriding `global.apps.config.<applicationID>.resources`.\n\n## Pod replica count\n\nThe purpose of specifying a replica count is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.\n\nThe OpenShift and Kubernetes documentation provide more information about replicas:\n\n* OpenShift: [Replication controllers](https://docs.openshift.com/container-platform/4.7/applications/deployments/what-deployments-are.html#deployments-replicationcontrollers_what-deployments-are)\n* Kubernetes: [ReplicaSet](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)\n\nThe number of replicas in a deployment can be specified globally via `apps.replicaCount` or via `global.apps.config.<applicationID>.replicaCount`: where `<applicationID>` is replaced by the lower-case EAR file basename (e.g., `curam`, `citizenportal`, `rest`, etc).\n\nThe default is a single replica.\n\nFor example, an override file to specify 2 `curam` replicas and 4 `rest` replicas:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        replicaCount: 2\n      rest:\n        replicaCount: 4\n...\n```\n\nThe number of replicas can also be specified at a more granular level for producer and consumer deployments.\nFor example, an override file to specify three `curam producer` replicas and six `curam consumer` replicas:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          replicaCount: 3\n        consumerTuning:\n          replicaCount: 6\n...\n```\n\n## Pod requests and limits\n\nThe OpenShift and Kubernetes documentation provide more information about requests and limits:\n\n* OpenShift: [Resource quotas per project](https://docs.openshift.com/container-platform/4.7/applications/quotas/quotas-setting-per-project.html)\n* Kubernetes: [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)\n\nThe SPM Helm charts are coded to be able to specify `cpu` and `memory` requests and limits. For instance, this shows the default settings for `curam` application pods:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        resources:\n          limits:\n            cpu: 2\n            memory: 4Gi\n          requests:\n            cpu: 2\n            memory: 4Gi\n...\n```\n\n<InlineNotification kind=\"warning\">\n\nFor the Guaranteed QoS, SPM highly recommends that a pod must have **both** CPU and Memory requests and limits set to equal values. If requests & limits are different for a given pod, it would be eligible for rescheduling in case of resource pressure on its node.\n\nFor more information please review [Configure Quality of Service for Pods](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/#create-a-pod-that-gets-assigned-a-qos-class-of-guaranteed).\n\n</InlineNotification>\n\nThese requests and limits can also be specified at a more granular level for producer and consumer deployments.\n\nFor example:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          resources:\n            limits:\n              cpu: 2\n              memory: 3584Mi\n            requests:\n              cpu: 2\n              memory: 3584Mi\n        consumerTuning:\n          resources:\n            limits:\n              cpu: 2\n              memory: 2048Mi\n            requests:\n              cpu: 2\n              memory: 2048Mi\n...\n```\n\n## Liberty thread pool\n\nThe total number of threads that SPM uses in a producer pod can be set initially to `(requested_cpu * 2)`. Setting the number of threads to twice the number of cores is based on experience that processing in SPM is usually split relatively equally between I/O and CPU.\n\nThe SPM Helm charts allow for overriding the WebSphere Liberty executor thread pool minimum (`coreThreads`) and maximum (`maxThreads`) settings either globally\n(e.g. `apps.tuningDefaults.coreThreads`) or per application as per the [Initial Tuning Settings](https://ibm.github.io/spm-kubernetes/resources/tuning-values.yaml).\n\nFor example, a tuning specification of 8 threads for `curam` producer pods:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          coreThreads: 8\n          maxThreads: 8\n...\n```\n\nThe `coreThreads` and `maxThreads` values map to the pod's WebSphere Liberty configuration in `/config/server.xml` and the values are populated via `/config/server.env`.\n\nFor example:\n\n```xml\n<server>\n  ...\n  <executor coreThreads=\"${env.EX_CORE_THREADS}\" maxThreads=\"${env.EX_MAX_THREADS}\" />\n</server>\n```\n\n## Liberty JDBC configuration tuning\n\n### Data source: `jdbc/curamdb`\n\nA Social Program Management (SPM) transaction can require two JDBC connections, one for the transaction itself and another one for the key server.\n\nTherefore, size the `jdbc/curamdb` data source connection pool to prevent deadlocks, with more connections available than threads that SPM uses.\n\n### Data source: `jdbc/curamtimerdb`\n\nThe EJB timer service is used by all SPM transactions, but only once per transaction, in our application infrastructure and at the very start of an SPM transaction. Currently, no reference to or usage of this service exists after the very start of an SPM transaction.\n\nYou can tune the size of the jdbc/curamtimerdb data source connection pool to be the same size as the number of threads, which would ensure that no contention can occur on the pool.\nHowever, given that the time that is spent using the EJB timer service is typically short compared to the duration of the transactions, a smaller size should work well without significant contention.\nSo our advice is to start with the default size, monitor the system, and then increase the size if evidence exists of a significant contention under normal conditions.\n\n### Statement Cache Size\n\nAs a starting value for SPM, set the data source prepared statement cache size for `jdbc/curamdb` to `1000`.\nThen, monitor the cache use and increase it if discards occur. In our experience, preventing discards can increase throughput by up to 20%.\n\n<InlineNotification>\n\n**Note:** While we recommend an initial value of 1000 for the prepared statement cache to prevent discards,\nthis value can be too high for SPM-based systems that have many threads and that are memory constrained.\nIn that case, it is recommended to review the SQLStats from the JMX Stats, from either load tests or production.\nThen, use a simple heuristic based on the distribution of SQL executions to find a smaller cache size that covers around\n90% of SQL executions from the application and gives a better balance between system performance and Java™ heap utilization.\nHowever, monitor prepared statement cache discards, system performance, and heap utilization, and adjust the cache size further as needed.\n</InlineNotification>\n\n### JDBC configuration tuning\n\nThe WebSphere Liberty [JDBC configuration](https://www.ibm.com/docs/en/was-liberty/base?topic=configuration-datasource) is tunable for each of these database definitions:\n\n|Database definitions | Description |\n|:- | :- |\n|`curamdb`|Used by SPM applications|\n|`curamtimerdb` | Used by the [SPM timer infrastructure](https://www.ibm.com/docs/en/spm/8.0.0?topic=behavior-cram-timer) |\n|`curamsessdb`| Used for WebSphere Liberty's [HTTP session replication](/openshift/tune_jms_producer/tune_jms_producer#liberty-http-session-replication)|\n\nThe following yaml keys are provided for tuning the WebSphere Liberty JDBC configuration:\n\n* _maxPoolSize_ -  Maximum number of database connections; Helm chart default: `8`\n  * `curamdb_maxPoolSize` = `(( max_threads * 2 ) + 1))`\n  * `curamtimerdb_maxPoolSize` = `( max_threads + 1)`\n  * `curamsessdb_maxPoolSize`= `( max_threads + 1 )`\n* _numConnectionsPerThreadLocal_ - Number of connections to the database to be cached for each thread; Helm chart default: `2`\n\n  <InlineNotification>\n\n  **Note:** For SPM _numConnectionsPerThreadLocal_ should always be set to `2`. `1` connection for the `keyserver` and `1` connection for the SPM database\n\n  </InlineNotification>\n\n  * `curamdb_numConnectionsPerThreadLocal` = `2`\n  * `curamtimerdb_numConnectionsPerThreadLocal` = `2`\n  * `curamsessdb_numConnectionsPerThreadLocal` = `2`\n* _purgePolicy_ - Connections to be destroyed in the pool when a stale connection is detected; Helm chart default: `EntirePool`\n  * `curamdb_purgePolicy` = `EntirePool`\n  * `curamtimerdb_purgePolicy` = `EntirePool`\n  * `curamsessdb_purgePolicy` = `EntirePool`\n* _statementCacheSize_ - Maximum number of cached statements per connection; Helm chart default: `1000`\n  * `curamdb_statementCacheSize` = `1000`\n  * `curamtimerdb_statementCacheSize` = `1000`\n  * `curamsessdb_statementCacheSize` = `1000`\n\nThe SPM Helm charts allow for overriding the JDBC configuration either globally\n(e.g., `apps.tuningDefaults.curamdb_maxPoolSize`) or per application as per the [Initial Tuning Settings](https://ibm.github.io/spm-kubernetes/resources/tuning-values.yaml) examples provided.\nHere we illustrate tuning settings for curamdb in curam producer pods:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          # Curam Producer Database Settings\n          curamdb_maxPoolSize: 17\n          curamdb_numConnectionsPerThreadLocal: 2\n          curamdb_purgePolicy: EntirePool\n          curamdb_statementCacheSize: 1000\n\n          curamtimerdb_maxPoolSize: 9\n          curamtimerdb_numConnectionsPerThreadLocal: 2\n          curamtimerdb_purgePolicy: EntirePool\n          curamtimerdb_statementCacheSize: 1000\n\n          curamsessdb_maxPoolSize: 9\n          curamsessdb_numConnectionsPerThreadLocal: 2\n          curamsessdb_purgePolicy: EntirePool\n          curamsessdb_statementCacheSize: 1000\n...\n```\n\nThe various keys in the preceding list map to the pod's WebSphere Liberty configuration in `/config/adc_conf/server_resources_jdbc.xml` and the setting values are populated via `/config/server.env`.\nFor example, showing the relevant parts of the `curamdb` configuration:\n\n```xml\n<server>\n  <dataSource id=\"curamdb\" jndiName=\"jdbc/curamdb\"\n    statementCacheSize=\"${env.DS_CURAMDB_CACHE_SIZE}\">\n    <connectionManager\n      maxPoolSize=\"${env.CM_CURAMDB_MAX_POOL_SIZE}\"\n      numConnectionsPerThreadLocal=\"${env.CM_CURAMDB_CONN_PER_THREAD}\"\n      purgePolicy=\"${env.CM_CURAMDB_PURGE_POLICY}\"\n    />\n  </dataSource>\n</server>\n```\n\n## Liberty JMS configuration tuning\n\nThe WebSphere Liberty JMS configuration is tunable for the JMS [connection manager settings](https://www.ibm.com/docs/en/was-liberty/base?topic=configuration-jmsconnectionfactory#connectionManager) associated with the `CuramQueueConnectionFactory`\n\n### JMS connection manager tuning\n\nThe following JMS connection manager settings, associated with the `CuramQueueConnectionFactory`, can be tuned:\n\n|Connection manager setting|Yaml default comes from|Yaml key used if specific|Description|\n|--------------------------|-----------------------|-------------------------|-----------|\n|`maxPoolSize`|`max_thread + 1`|`apps.<applicationID>.producerTuning.mqMaxPoolSize`|Specifies the maximum number of physical connections for the connection pool|\n|`minPoolSize`|`max_thread + 1`|`apps.<applicationID>.producerTuning.mqMinPoolSize`|Specifies the minimum number of physical connections for the connection pool|\n|`numConnectionsPerThreadLocal`| `6`|`apps.<applicationID>.producerTuning.mqNumConnectionsPerThreadLocal`|Specifies the number of connections to cache for each executor thread|\n|`maxConnectionsPerThread`| `6`|`apps.<applicationID>.producerTuning.maxJMSConnectionsPerThread`|Limits the number of open connections on each thread|\n\nThe keys in the preceding table map to the pod's WebSphere Liberty configuration in `/config/adc_conf/server_resources_messaging.xml`\nand the setting values are populated via `/config/server.env` as shown in this WebSphere Liberty configuration fragment:\n\n```xml\n<server>\n  ...\n  <connectionManager\n    id=\"ConMgr6\"\n    maxPoolSize=\"${env.CM_MQ_MAXPOOLSIZE}\"\n    minPoolSize=\"${env.CM_MQ_MINPOOLSIZE}\"\n    numConnectionsPerThreadLocal=\"${env.CM_JMS_NUM_CONNECTIONS_PER_THREAD_LOCAL}\"\n    maxConnectionsPerThread=\"${env.CM_JMS_MAX_CONNECTIONS_PER_THREAD}\"\n  />\n</server>\n\n```\n\nThe SPM Helm charts allow for overriding the JMS configuration either globally (e.g. `apps.tuningDefaults.maxThreads`) or per application as per the [Initial Tuning Settings](https://ibm.github.io/spm-kubernetes/resources/tuning-values.yaml) examples provided.\nHere we illustrate tuning settings for curamdb in curam producer pods:\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          mqMaxPoolSize: 9\n          mqMinPoolSize: 9\n          mqNumConnectionsPerThreadLocal: 6\n          maxJMSConnectionsPerThread: 6\n...\n```\n\n## Liberty JVM heap\n\nWebSphere Liberty JVM options are specified via a yaml array in `global.apps.config.<applicationID>.jvm` for all pod types of an application or for specific pod types such as producer, via the `global.apps.config.<applicationID>.producerTuning`\tdictionary of tuning values.\n\n<InlineNotification>\n\n**Note:** SPM recommends minimum memory setting per producer pod of `3584Mi`.\n\n</InlineNotification>\n\nStart with the following settings:\n\n* For a given consumer pod where `memory requests` = `3584Mi`, tune the JVM heap size by using the following example:\n\n```\n  -Xmx = 2560M\n  -Xms = 2560M\n  -Xmn = 1536M\n```\n\nFragment showing JVM settings for the `curam` JMS producer pods (as distinct from the `curam` JMS producer pods):\n\n```yaml\nglobal:\n  apps:\n    config:\n      curam:\n        producerTuning:\n          jvm: ['-Xms2560M','-Xmx2560M','-Xmn1536M']\n```\n\nThese settings are placed in the pod's `/config/jvm.options` file at deployment, for instance:\n\n```\n-Xms2560M\n-Xmx2560M\n-Xmn1536M\n```\n\n## Liberty HTTP session replication\n\nSPM deployed in Kubernetes uses WebSphere Liberty HTTP session replication for failover.\nThis replication is done using the database as the persistence and sharing mechanism for HTTP sessions.\nIn our performance tests we have seen at least an order of magnitude improvement in service times from the REST producer pod by switching the write frequency of the HTTP session replication from \"End of Servlet service\" to \"Time based\".\n\nWhen using \"End of Servlet service\", for each HTTP request arriving to the pod, the HTTP session is first read from the database, then the SPM code is executed,\nand lastly the HTTP session is updated in the database before the HTTP response is sent.\nWhen using \"Time based\", HTTP requests incur a much smaller overhead as the HTTP sessions are updated in the database asynchronously to the HTTP requests.\n\nExample in context:\n\n```xml\n<server>\n  <httpSessionDatabase\n    ....\n    skipIndexCreation=\"false\"\n    writeFrequency=\"TIME_BASED_WRITE\"\n    writeInterval=\"2m\"\n  />\n</server>\n```\n","fileAbsolutePath":"/home/travis/build/IBM/spm-performance-tuning/src/pages/openshift/tune_jms_producer/tune_jms_producer.mdx"}}},
    "staticQueryHashes": ["1364590287","137577622","2102389209","2456312558","2610115429","2746626797","3037994772","768070550"]}