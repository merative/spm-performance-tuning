{
    "componentChunkName": "component---src-pages-tuneappserver-monitoring-appserver-mdx",
    "path": "/tuneappserver/monitoring-appserver/",
    "result": {"pageContext":{"frontmatter":{"title":"Monitoring the application server","description":"Monitoring the application server"},"relativePagePath":"/tuneappserver/monitoring-appserver.mdx","titleType":"page","MdxNode":{"id":"9a56bc7e-40a1-5fcc-9511-d3feeb51abe8","children":[],"parent":"db0dc856-9a5e-5074-bddf-8b10c92e6342","internal":{"content":"---\ntitle: Monitoring the application server\ndescription: Monitoring the application server\n---\n\nUse the tasks in the following section to monitor the system after you make the tuning changes.\n\n## JVM\n\nTo confirm and fine-tune the JVM heap size, turn on garbage collection (GC) logging by adding the following JVM parameter:\n\n```shell\n-verbose:gc\n```\n\nOn WebSphere® Application Server (WAS) with the IBM JVM, you can specify the location of the GC log file by setting the following JVM parameter:\n\n```shell\n-Xverbosegclog:<<path to file>>\n```\n\nWith a non-IBM JVM, you can specify the location by setting the following JVM parameter:\n\n```shell\n-Xloggc:<<path to file>>\n```\n\nYou can then process the GC log file to analyze the GC efficiency and identify better GC tuning values.\nAs a general convention, if more than 2% of the JVM time is spent doing garbage collection,\nadjust the heap size as described in the [JVM settings](/tuneappserver/jvm-settings) section.\n\n## JVM heap size for WebSphere Application Server Liberty\n\nTo get a heap dump if the JVM crashes, set the following JVM parameter:\n\n```shell\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$WLP_HOME/usr/servers/CuramServer/\n```\n\n`-XX:+HeapDumpOnOutOfMemoryError` generates a heap dump when an allocation from the Java™ heap or the permanent generation cannot be satisfied.\n\n## Threads\n\nMonitor thread utilization in the thread pools. For example, the `WebSphere Performance Viewer - “Active Thread”`\ncounter shows the number of threads that are used in a thread pool. You can compare the counter with the defined\nnumber of threads to determine whether a thread pool is fully used.\n\nIf a thread pool shows as fully used and if spare CPU capacity exists, you can add a thread to the thread pool.\nThen, use the formulas that are described previously to update the connection pool sizes to reflect the new number of threads.\nIf no spare CPU capacity exists, then you must balance the tuning to favor either online or asynchronous processing.\n\nConsider the case where the online thread pool is fully used, which results in poor user response times from the application server.\nYou might favor the online processing by decreasing the number of asynchronous threads by 1 and by increasing the number of online threads by 1.\n\n## JDBC\n\nMonitor the prepared statement cache discards for the `jdbc/curamdb` data source. For example,\nin WAS, monitor the `JDBC connection pools PrepStmtCacheDiscardCount` in the `WebSphere Performance Viewer -Extended Statistic Set`.\n\nNot reusing a prepared statement has a significant processing cost. Therefore, aim for no discards,\nand increase the prepared statement cache size if many discards are reported. If you are using the Oracle database,\nmonitor the maximum number of open cursors and update the configuration as needed.\n\n## JMS\n\nMonitor the depth of JMS queues at run time. It indicates how many messages are waiting to be processed.\nFor example, in WAS the `AvailableMessageCount` counter is available from the `WebSphere Performance Viewer-All Statistic-Queues-Queue Stat`.\nThe following list indicates the key queues to monitor:\n\n* `DPEnactment`\n* `WorkflowEnactment`\n* `WorkflowActivity`\n\nIf the rate of JMS message processing is not satisfactory and if spare CPU capacity exists,\nyou can increase the number of threads. Use either `SIBJMSRAThreadPool` for WAS or `MDBWorkManager` for WLS.\n\nHowever, if no spare CPU capacity exists, review either the maximum concurrent endpoints for the queues in WAS or the beans\nin the free pool in WLS. In this case, either constrain the queues in WAS or the WLS free pool beans that have a\nlow depth to favor those queues with a high depth. That is, either the queues or the beans that have high queue depths or message counts need more resources.\n\nFor more information about applying the constraints, see [WAS - activation specifications](/tuneappserver/jms-settings)\nor [WLS - message driven beans](/tuneappserver/jms-settings).\n","type":"Mdx","contentDigest":"195c3936a46dfb3dd8eef5c3339e0d84","owner":"gatsby-plugin-mdx","counter":115},"frontmatter":{"title":"Monitoring the application server","description":"Monitoring the application server"},"exports":{},"rawBody":"---\ntitle: Monitoring the application server\ndescription: Monitoring the application server\n---\n\nUse the tasks in the following section to monitor the system after you make the tuning changes.\n\n## JVM\n\nTo confirm and fine-tune the JVM heap size, turn on garbage collection (GC) logging by adding the following JVM parameter:\n\n```shell\n-verbose:gc\n```\n\nOn WebSphere® Application Server (WAS) with the IBM JVM, you can specify the location of the GC log file by setting the following JVM parameter:\n\n```shell\n-Xverbosegclog:<<path to file>>\n```\n\nWith a non-IBM JVM, you can specify the location by setting the following JVM parameter:\n\n```shell\n-Xloggc:<<path to file>>\n```\n\nYou can then process the GC log file to analyze the GC efficiency and identify better GC tuning values.\nAs a general convention, if more than 2% of the JVM time is spent doing garbage collection,\nadjust the heap size as described in the [JVM settings](/tuneappserver/jvm-settings) section.\n\n## JVM heap size for WebSphere Application Server Liberty\n\nTo get a heap dump if the JVM crashes, set the following JVM parameter:\n\n```shell\n-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=$WLP_HOME/usr/servers/CuramServer/\n```\n\n`-XX:+HeapDumpOnOutOfMemoryError` generates a heap dump when an allocation from the Java™ heap or the permanent generation cannot be satisfied.\n\n## Threads\n\nMonitor thread utilization in the thread pools. For example, the `WebSphere Performance Viewer - “Active Thread”`\ncounter shows the number of threads that are used in a thread pool. You can compare the counter with the defined\nnumber of threads to determine whether a thread pool is fully used.\n\nIf a thread pool shows as fully used and if spare CPU capacity exists, you can add a thread to the thread pool.\nThen, use the formulas that are described previously to update the connection pool sizes to reflect the new number of threads.\nIf no spare CPU capacity exists, then you must balance the tuning to favor either online or asynchronous processing.\n\nConsider the case where the online thread pool is fully used, which results in poor user response times from the application server.\nYou might favor the online processing by decreasing the number of asynchronous threads by 1 and by increasing the number of online threads by 1.\n\n## JDBC\n\nMonitor the prepared statement cache discards for the `jdbc/curamdb` data source. For example,\nin WAS, monitor the `JDBC connection pools PrepStmtCacheDiscardCount` in the `WebSphere Performance Viewer -Extended Statistic Set`.\n\nNot reusing a prepared statement has a significant processing cost. Therefore, aim for no discards,\nand increase the prepared statement cache size if many discards are reported. If you are using the Oracle database,\nmonitor the maximum number of open cursors and update the configuration as needed.\n\n## JMS\n\nMonitor the depth of JMS queues at run time. It indicates how many messages are waiting to be processed.\nFor example, in WAS the `AvailableMessageCount` counter is available from the `WebSphere Performance Viewer-All Statistic-Queues-Queue Stat`.\nThe following list indicates the key queues to monitor:\n\n* `DPEnactment`\n* `WorkflowEnactment`\n* `WorkflowActivity`\n\nIf the rate of JMS message processing is not satisfactory and if spare CPU capacity exists,\nyou can increase the number of threads. Use either `SIBJMSRAThreadPool` for WAS or `MDBWorkManager` for WLS.\n\nHowever, if no spare CPU capacity exists, review either the maximum concurrent endpoints for the queues in WAS or the beans\nin the free pool in WLS. In this case, either constrain the queues in WAS or the WLS free pool beans that have a\nlow depth to favor those queues with a high depth. That is, either the queues or the beans that have high queue depths or message counts need more resources.\n\nFor more information about applying the constraints, see [WAS - activation specifications](/tuneappserver/jms-settings)\nor [WLS - message driven beans](/tuneappserver/jms-settings).\n","fileAbsolutePath":"/home/travis/build/IBM/spm-performance-tuning/src/pages/tuneappserver/monitoring-appserver.mdx"}}},
    "staticQueryHashes": ["1364590287","137577622","2102389209","2456312558","2610115429","2746626797","3037994772","768070550"]}